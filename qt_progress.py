# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'dc4_1.ui'
#
# Created by: PyQt5 UI code generator 5.15.9
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


import time

from PyQt5.QtCore import QThread, QObject, pyqtSignal
from PyQt5.QtWidgets import QMainWindow

from detect_utils import *


class Ui_SplashScreen(object):
    def setupUi(self, SplashScreen):
        SplashScreen.setObjectName("SplashScreen")
        SplashScreen.resize(717, 438)
        self.centralwidget = QtWidgets.QWidget(SplashScreen)
        self.centralwidget.setObjectName("centralwidget")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.centralwidget)
        self.verticalLayout.setContentsMargins(10, 10, 10, 10)
        self.verticalLayout.setSpacing(0)
        self.verticalLayout.setObjectName("verticalLayout")
        self.dropShadowFrame = QtWidgets.QFrame(self.centralwidget)
        self.dropShadowFrame.setStyleSheet("QFrame {    \n"
                                           "    background-color: rgb(56, 58, 89);    \n"
                                           "    color: rgb(220, 220, 220);\n"
                                           "    border-radius: 10px;\n"
                                           "}")
        self.dropShadowFrame.setFrameShape(QtWidgets.QFrame.StyledPanel)
        self.dropShadowFrame.setFrameShadow(QtWidgets.QFrame.Raised)
        self.dropShadowFrame.setObjectName("dropShadowFrame")
        self.label_title = QtWidgets.QLabel(self.dropShadowFrame)
        self.label_title.setGeometry(QtCore.QRect(20, 130, 661, 61))
        font = QtGui.QFont()
        font.setFamily("新宋体")
        font.setPointSize(40)
        self.label_title.setFont(font)
        self.label_title.setStyleSheet("color: rgb(254, 121, 199);")
        self.label_title.setAlignment(QtCore.Qt.AlignCenter)
        self.label_title.setObjectName("label_title")
        self.progressBar = QtWidgets.QProgressBar(self.dropShadowFrame)
        self.progressBar.setGeometry(QtCore.QRect(70, 200, 561, 21))
        self.progressBar.setStyleSheet("QProgressBar {\n"
                                       "    \n"
                                       "    background-color: rgb(98, 114, 164);\n"
                                       "    color: rgb(200, 200, 200);\n"
                                       "    border-style: none;\n"
                                       "    font: 12pt \"宋体\";\n"
                                       "    border-radius: 10px;\n"
                                       "    text-align: center;\n"
                                       "}\n"
                                       "QProgressBar::chunk{\n"
                                       "    border-radius: 10px;\n"
                                       "background-color: qlineargradient(spread:pad, x1:0, y1:0.511364, x2:1, "
                                       "y2:0.523, stop:0 rgba(254, 121, 199, 255), stop:1 rgba(170, 85, 255, 255));\n "
                                       "}")
        self.progressBar.setProperty("value", 0)
        self.progressBar.setObjectName("progressBar")
        self.label_loading = QtWidgets.QLabel(self.dropShadowFrame)
        self.label_loading.setGeometry(QtCore.QRect(20, 240, 661, 21))
        font = QtGui.QFont()
        font.setFamily("Segoe UI")
        font.setPointSize(12)
        self.label_loading.setFont(font)
        self.label_loading.setStyleSheet("color: rgb(98, 114, 164);")
        self.label_loading.setAlignment(QtCore.Qt.AlignCenter)
        self.label_loading.setObjectName("label_loading")
        self.btn_cancel = QtWidgets.QPushButton(self.dropShadowFrame)
        self.btn_cancel.setGeometry(QtCore.QRect(290, 280, 111, 41))
        self.btn_cancel.setStyleSheet("font: 14pt \"宋体\";\n"
                                      "color: rgb(254, 121, 199);\n"
                                      "background-color: rgb(255, 255, 255);")
        self.btn_cancel.setObjectName("btn_cancel")
        self.verticalLayout.addWidget(self.dropShadowFrame)
        SplashScreen.setCentralWidget(self.centralwidget)

        self.retranslateUi(SplashScreen)
        QtCore.QMetaObject.connectSlotsByName(SplashScreen)

    def retranslateUi(self, SplashScreen):
        _translate = QtCore.QCoreApplication.translate
        SplashScreen.setWindowTitle(_translate("SplashScreen", "MainWindow"))
        self.label_title.setText(_translate("SplashScreen", "<html><head/><body><p><span style=\" "
                                                            "font-size:18pt;\">系统正在处理视频文件，请稍后...</span></p></body"
                                                            "></html>"))
        self.label_loading.setText(_translate("SplashScreen", "加载中..."))
        self.btn_cancel.setText(_translate("SplashScreen", "取消"))
        self.loder_window(SplashScreen)

    def loder_window(self, SplashScreen):
        self.window = SplashScreen
        self.window.setAttribute(QtCore.Qt.WA_TranslucentBackground)
        self.window.setWindowFlags(QtCore.Qt.FramelessWindowHint)


from qt_check import *


class Worker(QObject):
    # 定义一个自定义信号，用于传递进度信息
    progress_signal = pyqtSignal(int)

    def __init__(self, check1, check2, check3, check4, video_path):
        super().__init__()
        self.total_frames = 50
        self.check1 = check1
        self.check2 = check2
        self.check3 = check3
        self.check4 = check4
        self.video_path = video_path
        self.stop_signal = False

        # 记录run处理函数的返回值
        self.abnormal_actions = []
        self.result_video_with_skeleton = ''
        self.result_video_without_skeleton = ''

    # test
    # def process_data(self):
    #     for step in range(30 + 1):
    #         if self.stop_signal is True:
    #             return
    #
    #         # 模拟耗时操作
    #         # 实际应用中，此处应替换为实际的数据处理逻辑
    #         time.sleep(0.1)  # 假设每步需要一些时间
    #
    #         # 发送当前进度信号
    #         self.progress_signal.emit(step * 100 // 30)

    # 安放RUN函数
    def process_data(self):
        # 类内传参
        video_path = self.video_path
        use_cdm = self.check1
        use_resnet = self.check3
        use_bp = self.check2
        use_gcn = self.check4

        vid_name = video_path.split("/")[-1][:-4]
        TEMP_IMG_PATH = './gcn/dataset/myDataset/tmp_img.jpg'
        class_names = ["no cheat", "cheat"]
        device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        inp_dets = 384
        max_age = 10
        weights_vote = [0.1, 0.15, 0.25, 0.5]

        use = [int(use_cdm), int(use_resnet), int(use_bp), int(use_gcn)]
        s = sum([weights_vote[i] * use[i] for i in range(4)])
        weights_vote = [weights_vote[i] / s for i in range(4)]

        fix = ' '.join(map(str, use))
        save_vid_wk = f"./video/{vid_name}_{fix}_1.mp4"
        save_vid_wtk = f"./video/{vid_name}_{fix}_0.mp4"

        # 检测模型
        detect_model = DetectAPI()
        # 骨骼点检测模型
        pose_model = MoveNet()
        # 坐标判定模型
        cdm_model = KeypointsChecker_CDM(rotor_thres=0.20, roll_thres=90, reach_thres=160)
        # 图像分类模型 cnn
        resnet_model = ResNetChecker(device=device, weights='./weights/myResNet_34_best.pt')
        # 骨骼点分类模型 dnn
        bp_model = KeypointsChecker_NN(device=device, weights='./weights/mymodel.pth')
        # gcn 时空图卷积分类
        action_model = TSSTG(weight_file="./weights/tsstg-model.pth",
                             class_names=class_names)
        resize_fn = ResizePadding(inp_dets, inp_dets)

        tracker = Tracker(max_age=max_age, n_init=3)

        cap = cv2.VideoCapture(video_path)

        frame_rate = cap.get(5)
        frame_num = cap.get(7)
        total_time = frame_num / frame_rate  # min

        codec = cv2.VideoWriter_fourcc(*'MP4V')
        writer = cv2.VideoWriter(save_vid_wk, codec, int(frame_rate / div), frameSize=(inp_dets * 2, inp_dets * 2))
        writer_n = cv2.VideoWriter(save_vid_wtk, codec, int(frame_rate / div), frameSize=(inp_dets * 2, inp_dets * 2))

        fps_time = 0
        f = 0
        cheat_ts = []
        while cap.isOpened():
            if self.stop_signal is True:
                return

            f += 1
            ret, frame = cap.read()
            if not ret:
                break
            frame = resize_fn(frame)
            if f == frame_num:
                break
            if f % div != 0:
                # writer.write(frame)
                # writer_n.write(frame)
                self.progress_signal.emit(f * 100 // frame_num)
                continue
            cv2.imwrite(TEMP_IMG_PATH, frame)
            frame = cv2.imread(TEMP_IMG_PATH)
            img_w, img_h = frame.shape[1], frame.shape[0]

            # Detect humans bbox in the frame with detector model.
            detected = detect_model.detect(TEMP_IMG_PATH)

            # Predict each tracks bbox of current frame from previous frames information with Kalman filter.
            tracker.predict()

            frame_n = frame.copy()
            detections = []  # List of Detections object for tracking.
            if detected is not None:
                bbs = np.array(detected[:, :4]).astype(int)
                confs = np.array(detected[:, 4])
                # Predict skeleton pose of each bboxs.
                keypoints_l, bbox_scores_l, bbox_l, keypoints2_l, keypoints3_l, cut_img_l = [], [], [], [], [], []
                for ib, (bb, conf) in enumerate(zip(bbs, confs)):
                    xx, yy, w, h = bb
                    x1, y1, x2, y2 = max(int(xx - w / 2), 0), max(int(yy - h / 2), 0), min(int(xx + w / 2),
                                                                                           img_w - 1), min(
                        int(yy + h / 2), img_h - 1)
                    cut_img = frame[y1: y2, x1: x2, :].copy()
                    bb = torch.tensor((xx - w / 2, yy - h / 2, xx + w / 2, yy + h / 2))
                    ch, cw, _ = cut_img.shape

                    keypoints = pose_model.run(cut_img)[0][0]

                    pos_in_img_init = torch.zeros(size=(17, 2))
                    scores = torch.zeros(size=(17, 1))
                    for ik, keypoint in enumerate(keypoints):
                        x, y, confidence = keypoint
                        x_pixel = int(x * ch)
                        y_pixel = int(y * cw)
                        pos_in_img_init[ik] = torch.tensor([bb[0] + y_pixel, bb[1] + x_pixel])
                        scores[ik] = torch.tensor([confidence])

                    pos_in_img = torch.cat([pos_in_img_init[0:1], pos_in_img_init[5:]], axis=0)
                    scores = torch.cat([scores[0:1], scores[5:]], axis=0)

                    bbox_l.append(bb)
                    bbox_scores_l.append(torch.tensor(conf))
                    keypoints_l.append(torch.cat([pos_in_img, scores], dim=1))
                    keypoints2_l.append(torch.tensor(pos_in_img_init))
                    keypoints3_l.append(torch.tensor(keypoints))
                    cut_img_l.append(cut_img)

                # Create Detections object.
                detections = [Detection(kpt2bbox(keypoints_l[ips][:, :2].numpy()),
                                        keypoints_l[ips].numpy(),
                                        keypoints_l[ips][:, 2].mean().numpy(),
                                        keypoints2_l[ips],
                                        keypoints3_l[ips],
                                        cut_img_l[ips]) for ips in range(len(bbox_l))]

            # Update tracks by matching each track information of current and previous frame or
            # create a new track if no matched.
            tracker.update(detections)

            # Predict Actions of each track.
            for i, track in enumerate(tracker.tracks):
                if not track.is_confirmed():
                    continue

                cf_keypoints_cdm = track.cf_keypoints_cdm
                cf_keypoints_dnn = track.cf_keypoints_dnn
                cf_cut_img = track.cf_cut_img

                resnet_res, bp_res, cdm_res = None, None, None
                # 图像分类
                if use_resnet:
                    resnet_res = resnet_model.run(cf_cut_img).argmax() if cf_cut_img is not None else None
                # 骨骼点分类
                if use_bp:
                    bp_res = bp_model.run(cf_keypoints_dnn).argmax() if cf_keypoints_dnn is not None else None
                # 坐标判定法
                if use_cdm:
                    cdm_res, _ = cdm_model.run(cf_keypoints_cdm) if cf_keypoints_cdm is not None else None

                track_id = track.track_id
                bbox = track.to_tlbr().astype(int)
                center = track.get_center().astype(int)

                # Use 30 frames time-steps to prediction.
                gcn_res = None
                if len(track.keypoints_list) == 10:
                    pts = np.array(track.keypoints_list, dtype=np.float32)
                    out = action_model.predict(pts, frame.shape[:2])
                    gcn_res = out[0].argmax()

                # 综合预测结果
                result = 0
                pred = [cdm_res, resnet_res, bp_res, gcn_res]
                nu = 0
                for j in range(4):
                    if pred[j] is not None and use[j]:
                        result += weights_vote[j] * pred[j]
                        nu += 1
                if nu != 0:
                    result_ = round(result)
                    action_name = action_model.class_names[result_]
                    confidence = result if result >= 0.5 else 1 - result
                    action = '{}: {:.2f}%'.format(action_name, confidence * 100)
                    if action_name == 'cheat':
                        clr = (0, 0, 255)
                    elif action_name == 'no cheat':
                        clr = (255, 200, 0)

                    # 若为作弊，则将作弊时刻进行记录
                    if result_ == 1:
                        cur_time = f / frame_rate
                        ch = int(cur_time // 3600)
                        cm = int(cur_time % 3600 // 60)
                        cs = int(cur_time % 60)
                        # cheat_ts.append(cur_time)
                        cheat_ts.append(f"{ch}:{cm}:{cs}")
                else:
                    action_name = 'pending..'
                    clr = (0, 255, 0)
                    action = '{}%'.format(action_name)

                # VISUALIZE.
                if track.time_since_update == 0:
                    frame = draw_single(frame, track.keypoints_list[-1])
                    # frame = cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 1)
                    frame = cv2.putText(frame, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX,
                                        0.4, (255, 0, 0), 2)
                    frame = cv2.putText(frame, action, (bbox[0] + 5, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX,
                                        0.4, clr, 1)
                    frame_n = cv2.putText(frame_n, str(track_id), (center[0], center[1]), cv2.FONT_HERSHEY_COMPLEX,
                                          0.4, (255, 0, 0), 2)
                    frame_n = cv2.putText(frame_n, action, (bbox[0] + 5, bbox[1] + 15), cv2.FONT_HERSHEY_COMPLEX,
                                          0.4, clr, 1)

            # Show Frame.
            frame = cv2.resize(frame, (0, 0), fx=2., fy=2.)
            frame = cv2.putText(frame, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),
                                (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            frame_n = cv2.resize(frame_n, (0, 0), fx=2., fy=2.)
            frame_n = cv2.putText(frame_n, '%d, FPS: %f' % (f, 1.0 / (time.time() - fps_time)),
                                  (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            # frame = frame[:, :, ::-1]
            fps_time = time.time()

            writer.write(frame)
            writer_n.write(frame_n)
            # cv2.imshow('frame', frame_n)
            # if cv2.waitKey(1) & 0xFF == ord('q'):
            #     break

            self.progress_signal.emit(f * 100 // frame_num)

        cheat_ts = list(OrderedDict.fromkeys(cheat_ts))
        writer.release()
        cv2.destroyAllWindows()

        self.abnormal_actions = cheat_ts
        self.result_video_with_skeleton = save_vid_wk
        self.result_video_without_skeleton = save_vid_wtk
        self.progress_signal.emit(f * 100 // frame_num)


class Progressbar(QtWidgets.QMainWindow):
    # 为了模拟模态窗口 即在视频处理过程中禁止用户任意触发其他按钮
    # 由于基于SplashScreen的设计 从多线程的角度允许用户同时处理多个视频或单个视频尝试多种处理办法(FAIL)
    # 不过此信号的模拟依旧保留(PICK)
    finished = pyqtSignal()
    # 用于强行终止的信号
    stop_signal = pyqtSignal()

    def __init__(self, check1, check2, check3, check4, video_path, key, dict, out_videos):
        super(Progressbar, self).__init__()
        self.ui = Ui_SplashScreen()
        self.ui.setupUi(self)
        # 传递给下一个页面
        self.check1 = check1
        self.check2 = check2
        self.check3 = check3
        self.check4 = check4

        self.key = key
        self.dict = dict
        self.out_videos = out_videos

        # 主页需要向后传递四个方法选择的bool状态以及选择的视频文件路径
        self.new_window = None

        self.worker_thread = QThread()
        self.data_worker = Worker(check1, check2, check3, check4, video_path)

        self.stop_signal.connect(self.stop_signal_triggered)
        self.data_worker.moveToThread(self.worker_thread)
        self.data_worker.progress_signal.connect(self.update_progress)

        self.ui.btn_cancel.clicked.connect(self.cancel_progress)

    def update_progress(self, value):
        self.ui.progressBar.setValue(value)
        if value >= 100:
            self.ui.window.close()
            parameter1 = self.data_worker.abnormal_actions
            parameter2 = self.data_worker.result_video_with_skeleton
            parameter3 = self.data_worker.result_video_without_skeleton

            self.worker_thread.quit()
            # 保存到映射中
            self.dict[self.key] = [parameter1, parameter2, parameter3]
            self.out_videos.append(parameter2)
            self.out_videos.append(parameter3)

            self.new_window = Check(self.check1, self.check2, self.check3, self.check4, parameter1, parameter2,
                                    parameter3, self.finished)
            self.new_window.show()

    def start_processing(self):
        self.worker_thread.started.connect(self.data_worker.process_data)
        self.worker_thread.start()

    def cancel_progress(self):
        # 当用户不想训练时可以强行取消
        # 这里点击取消原窗口的交换性限制也需要取消
        self.stop_signal.emit()
        self.worker_thread.quit()
        self.finished.emit()
        self.close()

    def stop_signal_triggered(self):
        # 触发stop_signal信号
        self.data_worker.stop_signal = True
